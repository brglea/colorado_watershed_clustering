{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Land Cover Classification at the Colorado River Basin**\n",
    "\n",
    "<figure>\n",
    "    <img\n",
    "        src=\"https://waterknowledge.colostate.edu/wp-content/uploads/sites/32/2018/06/CO-4-Major-Watersheds-resized.jpg\"\n",
    "        alt=\"Four Major Watersheds of Colorado Map, courtesy of CSU - Colorado Water Center, 2017.\" \n",
    "        height=\"400px\"/>\n",
    "    <img\n",
    "        src=\"https://www.usbr.gov/ColoradoRiverBasin/images/B557-800-23.jpeg\"\n",
    "        alt=\"Colorado River Basin, courtesy of U.S. Bureau of Reclamation 2025.\" \n",
    "        height=\"400px\"/>\n",
    "    <figcaption aria-hidden=\"true\">\n",
    "        Four Major Watersheds of Colorado Map, courtesy of CSU - Colorado Water \n",
    "        Center, 2017 (left). Colorado River Basin, courtesy of U.S. Bureau of \n",
    "        Reclamation 2025 (right).\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Overview**\n",
    "\n",
    "Known as the 'headwater state', Colorado is an important \n",
    "area in the U.S. for watersheads (CSU - Colorado Water \n",
    "Center 2017). The Colorado River Basin is of particular \n",
    "importance as it provides water to 7 states in the western \n",
    "U.S. (Colorado Department of Natural Resources, Colorado \n",
    "Water Conservation Board n.d.). In this project the Colorado \n",
    "River Basin main headwater areas will be the site of choice to \n",
    "perform k-means clustering for land cover. This is a wide expanse \n",
    "of land and has many different land covers that are known; however, \n",
    "k-means clustering as an unsupervised classification type provides \n",
    "a base of understanding broadly grouped land cover types.\n",
    "\n",
    "The Colorado River Headwaters come from different bodies of water, but \n",
    "the ones that will be focused on here are the Gunnison River to the south, \n",
    "the Colorado River that is more central, and then the Yampa River to the \n",
    "north. These three rivers encompass most of the water flow going out \n",
    "of the state to the west (CSU - Colorado Water Center 2017). The areas \n",
    "that contain these rivers were chosen because they encompass most of \n",
    "that flow.\n",
    "\n",
    "Using watershed boundary data and NASA HLS V2 satellite imagery, k-means \n",
    "algorithm will be applied to the site of choice to explore land cover \n",
    "clusters in the Colorado River Basin Headwaters.\n",
    "\n",
    "### Citations:\n",
    "\n",
    "* Colorado Department of Natural Resources, Colorado Water \n",
    "Conservation Board. n.d. “Colorado River Basin.” Cwcb.colorado.gov. \n",
    "State of Colorado, Department of Natural Resources. Accessed March \n",
    "16, 2025. https://cwcb.colorado.gov/colorado-river-basin.\n",
    "\n",
    "* CSU - Colorado Water Center. 2017. “Surface Water \n",
    "Resources | Colorado Water Knowledge | Colorado State \n",
    "University.” Waterknowledge.colostate.edu. Colorado State \n",
    "University. 2017. \n",
    "https://waterknowledge.colostate.edu/hydrology/surface-water-resources/#1528140197013-7bee1890-cc23.\n",
    "\n",
    "* U.S. Bureau of Reclamation. 2025. “Colorado River Basin.” Www.usbr.gov. \n",
    "U.S. Bureau of Reclamation. January 17, 2025. https://www.usbr.gov/ColoradoRiverBasin/.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Site Description\n",
    "\n",
    "<figure>\n",
    "    <img\n",
    "        src=\"https://d9-wret.s3.us-west-2.amazonaws.com/assets/palladium/production/s3fs-public/styles/full_width/public/thumbnails/image/Colorado%20Basin%20visual.JPG?itok=tRm1FXGy\"\n",
    "        alt=\"Colorado River Basin Map, courtesy of USGS 2019.\" \n",
    "        height=\"500px\"/>\n",
    "    <img\n",
    "        src=\"https://waterknowledge.colostate.edu/wp-content/uploads/sites/32/2018/08/CO-Surface-Water-Hydrology-CWCB-2017-800x507.png\"\n",
    "        alt=\"Colorado Surface Water Hydrology courtesy of CWCB 2017.\"  \n",
    "        height=\"500px\"/>\n",
    "    <figcaption aria-hidden=\"true\">\n",
    "        Colorado River Basin Map, courtesy of USGS 2019 (left). Colorado \n",
    "        Surface Water Hydrology courtesy of CWCB 2017 (right).\n",
    "    </figcaption>\n",
    "</figure>\n",
    "\n",
    "The Colorado River Basin is located in the southwestern U.S. between seven \n",
    "states that depend on it for \"water supply, hydropower production, recreation, \n",
    "fish and wildlife habitat\" and more (U.S. Bureau of Reclamation. n.d.). This \n",
    "area is about 250,000 square miles and includes the Colorado River (that eventually \n",
    "drains into the Gulf of Mexico) and its tributaries like the Gunnison River and \n",
    "the Yampa River (U.S. Bureau of Reclamation. n.d.). The area that the Colorado \n",
    "River basin occupies is shown in the above left map which also displays the state \n",
    "boundary lines and country boundary line with Mexico for orienting oneself to the area \n",
    "(CWCB 2017). The Colorado River Basin is divided into an upper and lower basins, \n",
    "and for this project since the focus is on the headwaters, the Upper Colorado \n",
    "River Basin will be of focus.\n",
    "\n",
    "The Colorado River Basin as a whole is very important to water supply both within \n",
    "the state of Colorado and outside of it (State of Colorado, DNR CWCB n.d.). This \n",
    "water supply supports \"agriculture, municipalities, outdoor recreation, hydropower \n",
    "generation, Tribal Nations, and drinking water\" (State of Colorado, DNR CWCB n.d.). \n",
    "However, this is not an unlimited natural resource and compounded severe drought years \n",
    "have left less water availble; there are currently ongoing negotiations for post 2026 \n",
    "agreements  (State of Colorado, DNR CWCB n.d.). \n",
    "\n",
    "For this project I chose the areas that are the major headwaters for the Colorado \n",
    "River Basin including the Colorado River and it's major tributaries with headwaters \n",
    "orginating in the Rocky Mountains (the Gunnison River, Yampa and White Rivers) \n",
    "(Water Recources Division USGS 1987). I wanted to specifically look at the majority \n",
    "of headwaters because of the significance they have culturally and their importance \n",
    "to the overall Colorado River Basin. Colorado is known as the headwater state and \n",
    "what that means is \"all of its rivers begin in the Rocky Mountains and flow out of \n",
    "the state\" (with very few exceptions) (CSU - Colorado Water Center 2017). Culturally \n",
    "there is great significance of the headwaters to first peoples - \"Since time \n",
    "immemorial Native Americans have had historical and cultural connections to water. \n",
    "For Tribes, water is life – It not only sustains them, supports agriculture and \n",
    "farming, native wildlife and riparian plants, food and sustenance, but is sacred to \n",
    "Tribal people.\" (Ten Tribes Partnership n.d.). Tribes have been historically \n",
    "excluded from talks about water rights and access, even though they are the original \n",
    "stewards of the land (Sakas 2021).\n",
    "\n",
    "A substatial amount of land in the areas I chose (the north, central and central south \n",
    "areas on the map on the right that show large blue flow arrows west) is federally owned with \n",
    "a handful of different land uses ranging from agriculture, grazing, recreation, etc. \n",
    "(CSU - Colorado Water Center 2017). A note of importance here is 'land use' is different \n",
    "from 'land cover'; land use is how land is being used like recreation or agriculture \n",
    "versus land cover is a physical land type like forest or desert (NOAA 2019). The predominant \n",
    "land cover types in the small portion of the Upper Colorado River Basin that I chose are:\n",
    "forest, shrubland, water (river related), grassland, developed (like cities), possibly \n",
    "others (CSU - Colorado Water Center 2017). This will be somewhat of a starting point for \n",
    "a number of ideal clusters, but I couldn't find a percise article or document depicting \n",
    "all of the land cover types present here. So, the silhouette score (which will be \n",
    "discussed in the methods section) will be helpful in getting a better picture of what an \n",
    "ideal number of clusters would be. \n",
    "\n",
    "\n",
    "### Site Description Citations:\n",
    "\n",
    "* CSU - Colorado Water Center. 2017. “Surface Water Resources | Colorado Water \n",
    "Knowledge | Colorado State University.” Waterknowledge.colostate.edu. Colorado State \n",
    "University. 2017. \n",
    "https://waterknowledge.colostate.edu/hydrology/surface-water-resources/#1528140197013-7bee1890-cc23.\n",
    "\n",
    "* Colorado Water Conservation Board [CWCB]. 2017. Colorado Average Streamflow \n",
    "Map. Water Diagrams. \n",
    "https://waterknowledge.colostate.edu/hydrology/surface-water-resources/#1528140242382-38dc7738-f2c9\n",
    "\n",
    "* National Oceanic and Atmospheric Administration (NOAA). 2019. “What Is the \n",
    "Difference between Land Cover and Land Use?” Noaa.gov. National Oceanic and \n",
    "Atmospheric Administration (NOAA). 2019. https://oceanservice.noaa.gov/facts/lclu.html.\n",
    "\n",
    "* Sakas, Michael Elizabeth. 2021. “Historically Excluded from Colorado River Policy, \n",
    "Tribes Want a Say in How the Dwindling Resource Is Used. Access to Clean Water Is a \n",
    "Start.” Colorado Public Radio. December 7, 2021. \n",
    "https://www.cpr.org/2021/12/07/tribes-historically-excluded-colorado-river-policy-use-want-say-clean-water-access/.\n",
    "\n",
    "* State of Colorado: Department of Natural Resources, Colorado Water Conservation Board. \n",
    "n.d. “Colorado River Basin | DNR CWCB.” Cwcb.colorado.gov. State of Colorado: Department of \n",
    "Natural Resources, Colorado Water Conservation Board. Accessed March 23, 2025. \n",
    "https://cwcb.colorado.gov/colorado-river-basin.\n",
    "\n",
    "* Ten Tribes Partnership. n.d. “Keepers of the River.” Keepers of the River. Ten \n",
    "Tribes Partnership. Accessed March 24, 2025. https://tentribespartnership.org/.\n",
    "\n",
    "* U.S. Bureau of Reclamation. n.d. “Colorado River Basin.” www.usbr.gov. U.S. Bureau \n",
    "of Reclamation. Accessed March 23, 2025. https://www.usbr.gov/ColoradoRiverBasin/.\n",
    "\n",
    "* United States Geological Survey (USGS). 2019. “Colorado River Basin - Water \n",
    "Users.” usgs.gov. United States Geological Survey (USGS). \n",
    "June 25, 2019. https://www.usgs.gov/media/images/colorado-river-basin-water-users.\n",
    "\n",
    "* Water Resources Division, U.S. Geological Survey. 1987. “River Basins of the United \n",
    "States: The Colorado,” 1–4. https://doi.org/10.3133/70039371."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "### * **Watershed Boundary Data**\n",
    "\n",
    "<figure>\n",
    "    <img\n",
    "        src=\"https://d9-wret.s3.us-west-2.amazonaws.com/assets/palladium/production/s3fs-public/styles/side_image/public/thumbnails/image/WBD_Base_HUStructure_small.png?itok=9AbwZ1zW\"\n",
    "        alt=\"Watershed Boundary Dataset Structure courtesy of USGS n.d.\" \n",
    "        height=\"600px\"/>\n",
    "    <figcaption aria-hidden=\"true\">\n",
    "        Watershed Boundary Dataset Structure courtesy of USGS n.d.\n",
    "    </figcaption>\n",
    "</figure>\n",
    "\n",
    "The Watershed Boundary Dataset (WBD) is one of a few hydrography related datasets \n",
    "that USGS provides (USGS n.d. \"Watershed Boundary Dataset\"). The WBD is a \n",
    "companion to other datsets they provide but can be used on its own, which will be \n",
    "the case in this project (USGS n.d. \"Watershed Boundary Dataset\"). The WBD is \n",
    "a \"national hydrologic unit dataset\" containing boundaries for hydrologic units \n",
    "(USGS n.d. \"Watershed Boundary Dataset\"). A hydologic unit \"defines the areal \n",
    "extent of surface water drainage to an outlet point on a dendritic stream network \n",
    "or to multiple outlet points where the stream network is not dendritic\" (USGS n.d. \n",
    "\"Watershed Boundary Dataset\"). The boundaries of these hydrologic units are \"based \n",
    "on topographic, hydrologic, and other relevant landscape characteristics without \n",
    "regard for administrative, political, or jurisdictional boundaries\", so state \n",
    "boundaries for example are not taken into account because those are arbitrary \n",
    "when thinking about hydrology for example (USGS n.d. \"Watershed Boundary Dataset\"). \n",
    "Hydrologic units can represent all or part of a \"total drainage area to a given \n",
    "outlet\", so for example, choosing a 12 digit hydorlogic unit level (which is \n",
    "particularly small - the data structure will be described in more detail in \n",
    "the next paragraph), of a portion of the Gunnison River, would only encompass \n",
    "a small part of the total drainage area to the Colorado River and multiple \n",
    "different 12 digit hydologic units would be needed to represent the total \n",
    "drainage area (USGS n.d. \"Watershed Boundary Dataset\"). However, hydrologic \n",
    "unit 2 digit, '14' for example encompasses the whole Upper Colorado River Basin. \n",
    "So, whatever the area of interest is and depending on the research question, one \n",
    "will need to think about hydrologic units, what level, and the count of units \n",
    "needed, or that multiple hyrdologic units at a certain level may be needed depending \n",
    "on the project.\n",
    "\n",
    "This project uses (v2.3.1) of the WBD; WBD seems to go through periodic updates \n",
    "going from 2009 to present and USGS is currently on the 5th edition of the Federal \n",
    "Standards and Procedures for this dataset (Jones et al. 2022). This report, which \n",
    "can be found [here](https://pubs.usgs.gov/tm/11/a3/), details guidelines and best \n",
    "practices for the WBD, the data structure, data editing and quality assurance, and \n",
    "\"Coding and Naming Hydrologic Units\" (Jones et al. 2022). The coding and naming of \n",
    "the hydrologic units I found particularly helpful to understand the levels and trying \n",
    "to think through what hydrologic unit level I would want to use for my project. I \n",
    "chose the 4 digit hydrologic unit (HU) level because of the question I am researching \n",
    "is rather broad and I don't need the specificity of something like a 6 digit or 12 \n",
    "digit; however, that level of area specificity can be pulled if wanted depending on \n",
    "the project. In general the higher the HU digit count, the smaller the area - so \n",
    "inside a 4 digit HU level are 6 digit HU levels, and inside that are 8 digit HU \n",
    "levels, and so on - in shorter terms it's a hierarchical and nested data strucutre \n",
    "(Jones et al. 2022). The image at the top of this section gives a better visual of \n",
    "that concept (USGS n.d. \"Watershed Boundary Dataset\"). In order to get to the \n",
    "desired level of data the first step though is to identify the 2 digit unit \n",
    "national number, then pulling the HU level of choice related to that 2 digit number. \n",
    "Because I am looking at a portion of the Upper Colorado River Basin, I first need to \n",
    "call unit 14 in the dataset because that unit is the 'Upper Colorado' which contains \n",
    "the more specified area that I want. In order to know what the unit numbers are, I \n",
    "used \"The National Map Downloader\", where different datsets could be visualized \n",
    "(like WBD) and then zoomed in or out to see the different HU level numbers (USGS n.d. \n",
    "“The National Map Downloader V2.”). After looking at this, I used 3 different 4 digit \n",
    "HU's to encompass the manjority of headwaters in the Upper Colorado River Basin.\n",
    "\n",
    "The WBD is publically available and free to use. The data itself can be accessed \n",
    "[here](https://prd-tnm.s3.amazonaws.com/index.html?prefix=StagedProducts/Hydrography/WBD/HU2/).\n",
    "I used the 'Shape' version in my project, but Geopackage and Geodatabase can also be used\n",
    "(USGS 2023).\n",
    "\n",
    "### Watershed Boundary Data Citations: \n",
    "\n",
    "* Jones, K.A., Niknami, L.S., Buto, S.G., and Decker, D., 2022, Federal standards \n",
    "and procedures for the national Watershed Boundary Dataset (WBD) (5 ed.): U.S. \n",
    "Geological Survey Techniques and Methods 11-A3, 54 p., \n",
    "https://pubs.usgs.gov/tm/11/a3/.\n",
    "\n",
    "* United States Geological Survey (USGS) . 2023. “The National Map: Staged Products \n",
    "Directory - Watershed Boundary Dataset.” prd-tnm.s3.amazonaws.com. United States \n",
    "Geological Survey (USGS). August 4, 2023. \n",
    "https://prd-tnm.s3.amazonaws.com/index.html?prefix=StagedProducts/Hydrography/WBD/HU2/Shape/.\n",
    "\n",
    "* United States Geological Survey (USGS). n.d. “The National Map Downloader V2.” \n",
    "Apps.nationalmap.gov. United States Geological Survey (USGS). Accessed March 23, 2025. \n",
    "https://apps.nationalmap.gov/downloader/#/.\n",
    "\n",
    "* United States Geological Survey (USGS). n.d. “Watershed Boundary Dataset.” Www.usgs.gov. \n",
    "United States Geological Survey (USGS). Accessed March 22, 2025. \n",
    "https://www.usgs.gov/national-hydrography/watershed-boundary-dataset.\n",
    "\n",
    "### * **NASA Harmonized Landsat Sentinel-2 - Multispectral Data**\n",
    "\n",
    "<figure>\n",
    "    <img\n",
    "        src=\"https://landsat.gsfc.nasa.gov/wp-content/uploads/2024/04/NASA_HLS_Thumb-1.png\"\n",
    "        alt=\"Image of Harmonized Landsat Sentinel-2 graphic. Provided by NASA\" \n",
    "        height=\"250px\"/>\n",
    "    <img\n",
    "        src=\"https://hls.gsfc.nasa.gov/wp-content/uploads/2023/09/Landsat89.Sentinel2AB.png\"\n",
    "        alt=\"Plot of wavelengths captured from Harmonized Landsat Sentinel-2 NASA\" \n",
    "        height=\"400px\"/>\n",
    "    <figcaption aria-hidden=\"true\">\n",
    "        Harmonized Landsat Sentinel-2 graphic and plot of wavelengths, courtesy of NASA.\n",
    "    </figcaption>\n",
    "</figure>\n",
    "\n",
    "HLS L30 (Harmonized Landsat and Sentinel-2) data, is the combined measurement \n",
    "from the NASA/USGS Landsat satellites (8 and 9) and the European Sentinels \n",
    "(2A and 2B) (USGS and NASA 2022). The combination of the two is what makes them \n",
    "'harmonized' and this also means there's more frequent observations (images) of \n",
    "the land being taken (every 2-3 days) rather than less frequent if used separately \n",
    "(Ju et al. n.d.). The user guide can be found \n",
    "[here](https://lpdaac.usgs.gov/documents/1698/HLS_User_Guide_V2.pdf) (Ju et al. n.d.).\n",
    "The L30 part is in reference to the images being at a special 30-meter spatial \n",
    "resolution (USGS and NASA 2022). These satellites have multiple sensors, each \n",
    "sensor is for a different reflective spectral band in the electromagnetic wavelength \n",
    "(SEOS and European Association of Remote Sensing Laboratories n.d.). A band alone \n",
    "cannot convey much, but the relationship between two or more bands does - these \n",
    "are normalized spectral indices such as NDMI, NDVI, and others (USGS n.d.). Information \n",
    "on more in detail about remote sensing can be found \n",
    "[here](https://seos-project.eu/remotesensing/remotesensing-c01-p06.html) \n",
    "(SEOS and European Association of Remote Sensing Laboratories n.d.). Bands layered can \n",
    "also output a True Color Image that uses the visible bands (red, green, and blue) which \n",
    "is what we typically associate with a photo we take with a camera or our phone, and a \n",
    "CIR (Color Infrared) Image (a.k.a. False Color Image) which uses different bands (Near \n",
    "Infrared - 'NIR', red, and green) (USGS 2022). The use of NIR here enhances the visualization \n",
    "of healthy vegetation which the NIR strongly reflects, making healthy vegetation appear \n",
    "bright red - basically enhances what the human eye cannot see (USGS 2022). \n",
    "\n",
    "To access the data [NASA Worldview](https://worldview.earthdata.nasa.gov) was used \n",
    "to search for western Colorado looking at the HLS L30 dataset (NASA 2025).\n",
    "A free account will need to be created to access this data and be able to \n",
    "use it in your codespace (NASA 2025). The module earthaccess and function search_data \n",
    "using parameters are used to search for the HLS L30 data according to \n",
    "the parameters set (earthaccess n.d.). Because this data comes from 'professional' \n",
    "sources (NASA, USGS, and European agencies) and gathered through satellites, \n",
    "there should be a degree of trust in the data, but it depends how you are \n",
    "using it and what you are using it for. For this project because land cover \n",
    "classification is the goal, the spectral signature of each pixel will be clustered \n",
    "into groups (clustering will be further discussed in the 'Methods' section below this). \n",
    "\n",
    "The data can vary depending on the day or days used (NASA 2025). For example, \n",
    "certain days have more or complete cloud coverage and it is essentially useless \n",
    "to for this project to pick that single day. It is best to get a 'clear' image \n",
    "or images where there is little to no cloud coverage for the best results.\n",
    "There will also be differences in the image based on time of year depending \n",
    "on what you are looking at. If interested in vegetation, it's important to \n",
    "know the climate and environment because you would want a time of year where \n",
    "the vegetation is healthiest and would have the greatest % reflection; however, \n",
    "it depends on what you are using this data to accomplish. In this project with \n",
    "hydrology being the focus particularly in headwater areas of the Upper Colorado \n",
    "River Basin, I am interested in the snow melt months which are generally between \n",
    "April and July (depending on several factors). In tandem to hydrology is vegetation \n",
    "in general; in theory I would want a vibrant time of year for vegetation like summer \n",
    "time (April to July should suffice) because the reflectance for vegetation would be \n",
    "high at this time. Therefore, it will be more clear where these vegetation types \n",
    "are to get a clear image and contrast different land cover types like water and \n",
    "vegetation, urban area, desert, etc..\n",
    "\n",
    "Singular dates can be chosen, a series of dates, or a time range for the temporal \n",
    "(datetime) parameter. Choosing a series of dates and esentially layering them to get \n",
    "100% coverage is what will be done here. The data provided on NASA Worldview can be \n",
    "used for many things beyond what this case study aims to do, and it is worth exploring \n",
    "(NASA 2025).\n",
    "\n",
    "### NASA HLS Citations:\n",
    "\n",
    "* earthaccess. n.d. “Earthaccess API: Search and Access .” Earthaccess.readthedocs.io. \n",
    "Accessed February 22, 2025. \n",
    "https://earthaccess.readthedocs.io/en/latest/user-reference/api/api/#earthaccess.api.search_data.\n",
    "\n",
    "* Ju, Junchang, Christopher Neigh, Martin Claverie, Sergii Skakun, Jean-Claude Roger, \n",
    "Eric Vermote, and Jennifer Dungan. n.d. “Harmonized Landsat Sentinel-2 (HLS) Product User \n",
    "Guide: Product Version 2.0.” NASA Harmonized Landsat and Sentinel-2, Product Description: L30. \n",
    "NASA. Accessed February 22, 2025. https://lpdaac.usgs.gov/documents/1698/HLS_User_Guide_V2.pdf.\n",
    "\n",
    "* NASA. 2023. “L30 – Harmonized Landsat Sentinel-2: Products Description.” \n",
    "Nasa.gov. 2023. https://hls.gsfc.nasa.gov/products-description/l30/.\n",
    "\n",
    "* NASA. 2025. “Worldview: Explore Your Dynamic Planet.” Nasa.gov. 2025.\n",
    "https://worldview.earthdata.nasa.gov.\n",
    "\n",
    "* NASA Goddard. 2024. “Data in Harmony: NASA’s Harmonized Landsat and Sentinel-2 Project.” \n",
    "YouTube. April 22, 2024. https://www.youtube.com/watch?v=63ljR84c85M.\n",
    "\n",
    "* NASA, and Michael P. Taylor. 2015. “Algorithms – Harmonized Landsat Sentinel-2.” NASA - Harmonized \n",
    "Landsat and Sentinel-2 . 2015. https://hls.gsfc.nasa.gov/algorithms/.\n",
    "\n",
    "* Science Education through Earth Observation for High Schools (SEOS), and European \n",
    "Association of Remote Sensing Laboratories. n.d. “Introduction to Remote Sensing.” \n",
    "Seos-Project.eu. Accessed February 22, 2025. \n",
    "https://seos-project.eu/remotesensing/remotesensing-c01-p06.html.\n",
    "\n",
    "* United States Geological Survey (USGS). 2022. “What Do the Different Colors in \n",
    "a Color-Infrared Aerial Photograph Represent? .” Www.usgs.gov. United States \n",
    "Geological Survey (USGS). February 1, 2022. \n",
    "https://www.usgs.gov/faqs/what-do-different-colors-color-infrared-aerial-photograph-represent.\n",
    "\n",
    "* United States Geological Survey (USGS), and NASA. 2022. “LP DAAC - HLSL30: \n",
    "Description.” USGS.gov. 2022. https://lpdaac.usgs.gov/products/hlsl30v002/.\n",
    "\n",
    "* United States Geological Survey (USGS). n.d. “Landsat Surface Reflectance-Derived \n",
    "Spectral Indices | U.S. Geological Survey.” Www.usgs.gov. USGS. Accessed February 22, 2025. \n",
    "https://www.usgs.gov/landsat-missions/landsat-surface-reflectance-derived-spectral-indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods Description\n",
    "\n",
    "K-means clustering algorithim will be utilized to find patterns in \n",
    "the raster data downloaded and processed from the HLS L30 data discussed \n",
    "in the above section, as applied to the study area chosen. These patterns \n",
    "are groups of pixels with similar spectral signatures and essentially \n",
    "each of these groups are 'clusters'. When this is plotted, it looks \n",
    "like a land cover map or visualization, but a note of importance is \n",
    "that this is not accurate depiction of reality of land cover on the ground.\n",
    "K-means is an exploratory method rather than predictive modeling (like \n",
    "habitat suitability models), or explanatory research (like the relationship \n",
    "of redlining grades to current urban greenspace). K-means also needs \n",
    "numerical or continuous data, and doesn't do well with binary or categorical \n",
    "data because k-means wouldn't be able to calculate the distance between \n",
    "variables. Some other important notes to remember about the k-means \n",
    "method is that it is unsupervised classification: there is no training \n",
    "data, accuracy cannot be assessed, even after applying it - one will \n",
    "not know what land cover type each cluster is, so it is best applied \n",
    "for broadly grouping categories like water and vegetation. If two \n",
    "pixels are in reality two differnt landcover types like grassland \n",
    "and shrubland, but they have similar spectral signatures, k-means \n",
    "would group them into the same cluster. So, this is why this method \n",
    "can't be relied on for one - knowing the actual landcover type, and \n",
    "two - that it's actually grouping distinct landcover types. This \n",
    "method is a hypothesis of the possible distribution of landcover types \n",
    "in an area, based on the number of clusters fed to the alogrithim.\n",
    "\n",
    "Using the k-means algorithim, a number of clusters will need to be \n",
    "set, this is an integer. The algorithim then divides the data into 'k' \n",
    "number of clusters via an optimization process that is then iterated through. \n",
    "First, it randomly selects data points as initial centroids, then assigns each \n",
    "data point to the closest centroid on the spectral band (rather than being \n",
    "assigned based on geography), then the algorithim updates the group centers \n",
    "to improve performance by computing the actual centroid of each group, which \n",
    "is then iterated through until performance converges. Because this algorithim \n",
    "at the very beginning randomly selects data points as centroids, means that \n",
    "different clusters or groups can be generated each time the algorithim is \n",
    "re-run. This is considered another limitation of this method and something to \n",
    "be aware of if you are choosing this method on your project.\n",
    "\n",
    "Another limitation to this method is that the number of clusters chosen for \n",
    "'k', changes the way the data is understood. The number of clusters is \n",
    "important because there is a saturation point at which increasing the number \n",
    "of clusters doesn't explain more of the data than if there were fewer clusters. \n",
    "It's a balance between knowing that each added cluster improves individual \n",
    "cluster performance, but the more clusters that are added, the harder it can \n",
    "be to interpret and have meaningful results. There are different ways to explore \n",
    "an ideal number of clusters. One of the ways is calculating a silhouette \n",
    "score and visualize that via an elbow plot. The silhouette score is the ratio \n",
    "(of variability) between the mean difference within each cluster and the \n",
    "mean distance to the nearest cluster. It is a way of understanding the \n",
    "quality of the clusters, and when looking at the elbow plot, you would make \n",
    "a personal decision for the number of clusters, which is ideally around where \n",
    "the 'elbow' or bend in the plot is. There are other methods to figure out the \n",
    "ideal number of clusters such as the average silhouette method and gap-static \n",
    "method, so there are other options to explore, but I will be using the elbow \n",
    "method in this project. \n",
    "\n",
    "\n",
    "An additional way to aid in land cover classification is to use principal \n",
    "component analysis (PCA), which is its own type of learning algorithim \n",
    "that helps with dimension reduction. PCA pulls out linear combinations of \n",
    "variables and forms independent variables. In this method one can see \n",
    "how much variation is explained by each component (a component is the \n",
    "independent variable it made), and you can see how much variation in the \n",
    "data can be explained by each component. So, if the first component explains \n",
    "99.99% of the variation in the data, one could try clustering based on \n",
    "this component/ independent variable, instead of using the original \n",
    "data's variables that may have been highly correlated. This is 'dimension \n",
    "reduction'. I will not be using this method in this project, but it is \n",
    "an available alogrithim to use in many types of analysis where dimension \n",
    "reduction would be useful. I don't find this method necessary given the \n",
    "data for this project, but if I were to choose a different dataset/project \n",
    "that I think needed it, I would likely use this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up analysis Part 1 of 2\n",
    "# Import libraries / packages\n",
    "\n",
    "# Basic Packages\n",
    "import os # Reproducible file paths\n",
    "from glob import glob # Find files by pattern\n",
    "import pathlib # Find the home folder\n",
    "import pickle # Serializing/unserializing objects\n",
    "import re # Regular expressions\n",
    "import warnings # Filter warning messages\n",
    "import zipfile # Work with zip files\n",
    "\n",
    "# Imports for spatial data\n",
    "import cartopy.crs as ccrs # CRSs (Coordinate Reference Systems)\n",
    "import earthaccess # Access NASA data from the cloud\n",
    "import earthpy as et # Spatial data analysis\n",
    "import geopandas as gpd # Work with vector data\n",
    "import geoviews as gv # Holoviews extension for data visualization\n",
    "import hvplot.pandas # Interactive tabular and vector data\n",
    "import hvplot.xarray # Interactive raster\n",
    "import matplotlib.pyplot as plt # Overlay pandas and xarry plots, Overlay raster and vector data\n",
    "import numpy as np # Adjust images \n",
    "import pandas as pd # Group and aggregate\n",
    "import rioxarray as rxr # Work with geospatial raster data\n",
    "import rioxarray.merge as rxrmerge # Merge rasters\n",
    "import xarray as xr # Adjust images\n",
    "from shapely.geometry import Polygon # Keep track of polygons geometry\n",
    "from sklearn.cluster import KMeans # kmeans clustering\n",
    "from sklearn.decomposition import PCA # Principal component analysis\n",
    "from sklearn.metrics import silhouette_score # Performance metrics\n",
    "from sklearn.preprocessing import StandardScaler # Standardize features\n",
    "import seaborn as sns # For plotting silhouette scores\n",
    "from tqdm.notebook import tqdm # Visualize progress of iterative operations\n",
    "\n",
    "# Import to be able to save plots\n",
    "import holoviews as hv # be able to save hvplots\n",
    "\n",
    "# Prevent GDAL from quitting due to momentary disruptions\n",
    "os.environ[\"GDAL_HTTP_MAX_RETRY\"] = \"5\"\n",
    "os.environ[\"GDAL_HTTP_RETRY_DELAY\"] = \"1\"\n",
    "\n",
    "# Suppress third party warnings - 'ignore'\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Clear progress bar results when finished\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Up Analysis Part 2 of 2\n",
    "\n",
    "# Define and create the project data directory\n",
    "co_landcover_cluster_data_dir = os.path.join(\n",
    "    pathlib.Path.home(),\n",
    "    'earth-analytics',\n",
    "    'co_land_cover_clustering'\n",
    ")\n",
    "# Make the directory\n",
    "os.makedirs(co_landcover_cluster_data_dir, \n",
    "            # If directory already exists no error is raised\n",
    "            exist_ok=True)\n",
    "\n",
    "# Call the data directory to check its location\n",
    "co_landcover_cluster_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a series of nested functions for caching a decorator\n",
    "# to use later in code, Part 1 of 1\n",
    "\n",
    "# Define function for the decorator\n",
    "def cached(func_key, override=False):\n",
    "    \"\"\"\n",
    "    A decorator to cache function results\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    key: str\n",
    "      File basename used to save pickled results\n",
    "    override: bool\n",
    "      When True, re-compute even if the results are already stored\n",
    "    \"\"\"\n",
    "    # Wrap caching function\n",
    "    def compute_and_cache_decorator(compute_function):\n",
    "        \"\"\"\n",
    "        Wrap the caching function\n",
    "        \n",
    "        Parameters\n",
    "        ==========\n",
    "        compute_function: function\n",
    "          The function to run and cache results\n",
    "        \"\"\"\n",
    "        # Computation and cache or load cached result\n",
    "        def compute_and_cache(*args, **kwargs):\n",
    "            \"\"\"\n",
    "            Perform a computation and cache, or load cached result.\n",
    "            \n",
    "            Parameters\n",
    "            ==========\n",
    "            args\n",
    "              Positional arguments for the compute function\n",
    "            kwargs\n",
    "              Keyword arguments for the compute function\n",
    "            \"\"\"\n",
    "\n",
    "            # Pickling and unpickling\n",
    "\n",
    "            # Add an identifier from the particular function call\n",
    "            # If 'cache_key' is found in kwargs\n",
    "            if 'cache_key' in kwargs:\n",
    "                # Create key by concatenating the 'func_key'\n",
    "                key = '_'.join((func_key, kwargs['cache_key']))\n",
    "            # If no'cache_key' is found in the kwargs    \n",
    "            else:\n",
    "                # Key will be set to the func_key\n",
    "                key = func_key\n",
    "\n",
    "            # Create file path for saving the cache\n",
    "            path = os.path.join(\n",
    "                # Specify root directory and location\n",
    "                et.io.HOME, et.io.DATA_NAME, 'jars',\n",
    "                # Pickle file named using 'key' \n",
    "                f'{key}.pickle')\n",
    "            \n",
    "            # Check if the cache exists already or override caching\n",
    "            # If cache does not exist\n",
    "            if not os.path.exists(path) or override:\n",
    "                # Make jars directory if needed\n",
    "                os.makedirs(os.path.dirname(path), \n",
    "                # If directory already exists no error is raised            \n",
    "                exist_ok=True)\n",
    "                # Run the compute function \n",
    "                result = compute_function(*args, **kwargs)\n",
    "                # Pickle the object and save it in a file\n",
    "                with open(path, 'wb') as file:\n",
    "                    pickle.dump(result, file)\n",
    "\n",
    "            # If cache already exists   \n",
    "            else:\n",
    "                # Unpickle the object\n",
    "                with open(path, 'rb') as file:\n",
    "                    # Load cached result\n",
    "                    result = pickle.load(file)\n",
    "            \n",
    "            # Return innermost function for computation and cache    \n",
    "            return result\n",
    "        \n",
    "        # Return wrap caching function\n",
    "        return compute_and_cache\n",
    "    \n",
    "    # Return decorator to cache function results\n",
    "    return compute_and_cache_decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Study Site (Watershed Boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select study site and plot Part 1 of \n",
    "# Create function to return a gdf of the watershed boundary data\n",
    "\n",
    "# Call the decorator function using the @\n",
    "@cached(\n",
    "        # The func_key here will be wbd_14\n",
    "        # this is for region 14 of the watershed boundary data\n",
    "        'wbd_14')\n",
    "# Create a new funtion, 3 parameters\n",
    "def read_wbd_file(wbd_filename, huc_level, cache_key):\n",
    "    \"\"\" \n",
    "        Download watershed boundary\n",
    "        Parameters:\n",
    "        -----------\n",
    "        wbd_filename: str\n",
    "            Name of file to be downloaded and processed\n",
    "        huc_level: int\n",
    "            Level of the data (Ex: 2, or 4, or 6, or 8, or 10, or 12)\n",
    "        cache_key: str\n",
    "            Can further refine huc level by the cache_key\n",
    "        Return:\n",
    "        -------\n",
    "        gdf of the watershed boundary \n",
    "    \"\"\"\n",
    "    # Download and unzip\n",
    "    # Define URL\n",
    "    wbd_url = (\n",
    "        \"https://prd-tnm.s3.amazonaws.com\"\n",
    "        \"/StagedProducts/Hydrography/WBD/HU2/Shape/\"\n",
    "        # Insert the name of the specific file wanted using f-string\n",
    "        f\"{wbd_filename}.zip\")\n",
    "    \n",
    "    # Define, create nested path and dir for watershed boundaries\n",
    "    # Path for watershed boundaries\n",
    "    watershed_bd_dir = os.path.join(co_landcover_cluster_data_dir, 'co_watershed_bd')\n",
    "    # Create directory for this path\n",
    "    os.makedirs(watershed_bd_dir, \n",
    "            # If directory already exists no error is raised\n",
    "            exist_ok=True)\n",
    "    # Download and unzip the data into the above dir\n",
    "    watershed_bd_dir = et.data.get_data(url=wbd_url)\n",
    "                  \n",
    "    # Read desired data\n",
    "    # Path to shapefile in dir, subfolder, shapefile\n",
    "    watershed_bd_path = os.path.join(watershed_bd_dir, 'Shape', f'WBDHU{huc_level}.shp')\n",
    "    # Read shapefile as a gdf\n",
    "    watershed_bd_gdf = gpd.read_file(watershed_bd_path, \n",
    "        # Use pyogrio library to read shapfile\n",
    "        engine='pyogrio')\n",
    "    \n",
    "    # Return the gdf of the watershed boundaries\n",
    "    return watershed_bd_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select study site and plot Part 2 of 4\n",
    "# Apply the read_wbd_file function\n",
    "\n",
    "# Define the huc_level - 4\n",
    "huc_level = 4\n",
    "# Download and open the shapefile\n",
    "co_wbd_gdf = read_wbd_file(\n",
    "    # Name of file to be downlaoded and processed\n",
    "    \"WBD_14_HU2_Shape\", \n",
    "    # Level of the data\n",
    "    huc_level, \n",
    "    # Refine by huc level\n",
    "    cache_key=f'hu{huc_level}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call gdf to see it\n",
    "co_wbd_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select study site and plot Part 3 of 4\n",
    "# Using 2 huc level 4 areas which are the headwaters for the \n",
    "# Colorado, Gunnison, and Yampa\n",
    "\n",
    "# Define new variable for the plot\n",
    "co_headwaters_gdf = (\n",
    "     # Filter the to the rows of areas wanted\n",
    "    co_wbd_gdf[co_wbd_gdf[f'huc{huc_level}']\n",
    "    # Select watershed/ area of interest \n",
    "    .isin(['1401'])] \n",
    "           #'1402', '1405'])]\n",
    "    # Merge geometries of all rows matching this watershed\n",
    "    .dissolve()\n",
    ")\n",
    "\n",
    "# Call the gdf to see it (should only have 1 row/ 1 geoometry)\n",
    "co_headwaters_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select study site and plot Part 4 of 4\n",
    "\n",
    "# # Plot site map of the watershed\n",
    "# cgy_headwaters_site_map = (\n",
    "# (\n",
    "#     # Project to Mercator \n",
    "#     co_headwaters_gdf.to_crs(ccrs.Mercator())\n",
    "#     # Plot using hvplot\n",
    "#     .hvplot(\n",
    "#         # Set geometry as slightly transparent and white\n",
    "#         alpha=.3, fill_color='white', \n",
    "#         # Add ESRI tile background\n",
    "#         tiles='EsriImagery', \n",
    "#         # Set basemap/background to Mercator to match\n",
    "#         crs=ccrs.Mercator(),\n",
    "#         # Set title\n",
    "#         title='Site Map: Coloarado Headwaters',\n",
    "#         )\n",
    "#     .opts(\n",
    "#         # Size of plot - set width and height\n",
    "#         width=600, height=700)\n",
    "# ))\n",
    "\n",
    "# # Save the plot as html to be able to display online\n",
    "# hv.save(cgy_headwaters_site_map, 'cgy_site_map.html')  \n",
    "# # Display the plot\n",
    "# co_headwaters_site_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select study site and plot Part 4 of 4\n",
    "\n",
    "# Plot site map of the watershed\n",
    "co_headwaters_site_map = (\n",
    "(\n",
    "    # Project to Mercator \n",
    "    co_headwaters_gdf.to_crs(ccrs.Mercator())\n",
    "    # Plot using hvplot\n",
    "    .hvplot(\n",
    "        # Set geometry as slightly transparent and white\n",
    "        alpha=.3, fill_color='white', \n",
    "        # Add ESRI tile background\n",
    "        tiles='EsriImagery', \n",
    "        # Set basemap/background to Mercator to match\n",
    "        crs=ccrs.Mercator(),\n",
    "        # Set title\n",
    "        title='Site Map: Coloarado Headwaters',\n",
    "        )\n",
    "    .opts(\n",
    "        # Size of plot - set width and height\n",
    "        width=600, height=400)\n",
    "))\n",
    "\n",
    "# Save the plot as html to be able to display online\n",
    "hv.save(co_headwaters_site_map, 'co_site_map.html')  \n",
    "# Display the plot\n",
    "co_headwaters_site_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Site Map Description: Visually diverse land cover\n",
    "\n",
    "The above site map includes three huc level 4 boundaries, but with \n",
    "their boundaries or geometries dissolved into one. The most south \n",
    "is the Gunnison River, roughly in the center is the Colorado River, \n",
    "and then northmost is the Yampa River. While the rivers cannot be seen \n",
    "clearly now, I am hoping that through clustering they will be more \n",
    "visible. The chosen area ranges roughly 4 degrees Longitude and 3 \n",
    "to 4 degrees Latitude. Most of the 'greener' areas are on the east \n",
    "side of this boundary which is part of the Rocky Mountains, then \n",
    "there are more aird areas to the west and north. This is not a super \n",
    "contained area compared to doing a single watershed, so any conclusions \n",
    "will be further encompassing, so some degree of specificity may be \n",
    "lost due to the scale. This area is visually diverse in tearms of land \n",
    "cover, so it will be interesting to use satellite imagry in the next step \n",
    "to further add to or build conclusions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Multispectral Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrangle raster data Part 1 of 4\n",
    "\n",
    "# Log in to earthaccess\n",
    "earthaccess.login(\n",
    "    # Prompt to input login credentials\n",
    "    strategy = \"interactive\", \n",
    "    # Remember credentials\n",
    "    persist = True)\n",
    "\n",
    "# Search for HLS tiles\n",
    "orig_co_hls_results = earthaccess.search_data(\n",
    "    # Filter search for data only related to HLS30\n",
    "    # HLS30 is Harmonized Landsat Sentinal data 30m resolution\n",
    "    short_name = \"HLSL30\",\n",
    "    # Only return cloud hosted data\n",
    "    cloud_hosted = True,\n",
    "    # Set spatial bounds\n",
    "    # Convert bounding box into tuple format for the search\n",
    "    bounding_box = tuple(\n",
    "        # Give min and max geogrpahic bounds of the gdf\n",
    "        co_headwaters_gdf.total_bounds),\n",
    "    # Set temporal bounds (rough snowmelt months)\n",
    "    temporal=(\"2024-04\", \"2024-06\")\n",
    ")\n",
    "\n",
    "# Call the variable to see if that worked\n",
    "# orig_co_hls_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(orig_co_hls_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrangle raster data Part 2 of 4\n",
    "\n",
    "# Create function to compile temporal and spatial \n",
    "# information about each granule in gdfs\n",
    "def get_earthaccess_links(results):\n",
    "    \"\"\"\n",
    "            Compile spatial and temporal info from granules into gdfs\n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            results: list\n",
    "              list of files (like geodataframes that were pulled from earthaccess)\n",
    "\n",
    "            Returns dataframe of gdfs from earthaccess results\n",
    "            \"\"\"\n",
    "    # Comppile a regular expression pattern into an object\n",
    "    url_re = re.compile(\n",
    "        # Create a regular expression\n",
    "        r'\\.(?P<tile_id>\\w+)\\.\\d+T\\d+\\.v\\d\\.\\d\\.(?P<band>[A-Za-z0-9]+)\\.tif')\n",
    "\n",
    "    # Loop through each granule\n",
    "    # Create empty list to save gdfs to\n",
    "    link_rows = []\n",
    "    # Create empty list to save the url dfs to\n",
    "    url_dfs = []\n",
    "    # Start for loop\n",
    "    # Have loop iterate through each granule in results\n",
    "    # Wrap results iterable in tqdm which displays progress bar\n",
    "    for granule in tqdm(results):\n",
    "        # Get granule information\n",
    "        # Get metadata info from each granule stored in the umm key for each granule\n",
    "        info_dict = granule['umm']\n",
    "        # Extract Granule Univeral Resource (UR) \n",
    "        # This is an identifier for each granule\n",
    "        granule_id = info_dict['GranuleUR']\n",
    "        # Extract datetime into pd object\n",
    "        # This helps with extracting specific parts like year, month, day\n",
    "        # Or calculating time difference or sorting/filtering by dates\n",
    "        datetime = pd.to_datetime(\n",
    "            # From the metadata of the granule\n",
    "            info_dict\n",
    "            # Extract temporal information using columns:\n",
    "            ['TemporalExtent']['RangeDateTime']['BeginningDateTime'])\n",
    "        # Extract boundary points of granule's spatial extent\n",
    "        points = (\n",
    "            # From the metadata of the granule\n",
    "            info_dict\n",
    "            # Extract spatial information using columns:\n",
    "            ['SpatialExtent']['HorizontalSpatialDomain']['Geometry']['GPolygons'][0]\n",
    "            ['Boundary']['Points'])\n",
    "        # Create polygon object\n",
    "        geometry = Polygon(\n",
    "            # Take lon lat pairs from the points above to construct a polygon\n",
    "            # that represents the granule's spatial boundary\n",
    "            [(point['Longitude'], point['Latitude']) for point in points])\n",
    "        \n",
    "        # Open granule files\n",
    "        files = earthaccess.open([granule])\n",
    "\n",
    "        # Build metadata DataFrame\n",
    "        for file in files:\n",
    "            # Apply regular expression\n",
    "            match = url_re.search(\n",
    "                # to the file.full_name\n",
    "                file.full_name)\n",
    "            # Conditional statement\n",
    "            # If match is found, this code block will execute\n",
    "            if match is not None:\n",
    "                # Each time a gdf is created append to the link_rows list\n",
    "                link_rows.append(\n",
    "                    # Create new gdf\n",
    "                    gpd.GeoDataFrame(\n",
    "                        # Create a dictionary with keys as columns\n",
    "                        dict(\n",
    "                            # Column names:\n",
    "                            # Extract datetime from file name\n",
    "                            datetime=[datetime],\n",
    "                            # Extract tile id from file name\n",
    "                            tile_id=[match.group('tile_id')],\n",
    "                            # Extract band from file name\n",
    "                            band=[match.group('band')],\n",
    "                            # Full url/file path of the data file\n",
    "                            url=[file],\n",
    "                            # Extract geomentry from the file\n",
    "                            geometry=[geometry]\n",
    "                        ),\n",
    "                        # Specify CRS for gdf based on lat lon\n",
    "                        crs=\"EPSG:4326\"\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    # Concatenate metadata DataFrame\n",
    "    # Concatenate list of gdfs from link_rows\n",
    "    file_df = pd.concat(\n",
    "        # Reset index to a default integer index\n",
    "        # This is because each gdf might have its own index which would\n",
    "        # make it hard to continue doing analysis on the data otherwise\n",
    "        link_rows).reset_index(\n",
    "            # Drop the old index column\n",
    "            # This keeps the old index column from being\n",
    "            # added as an additional column \n",
    "            drop=True)\n",
    "    # Return file_df which is the concatenated, reset list of gdfs created\n",
    "    return file_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_earthaccess_links(orig_co_hls_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrangle raster data Part 3 of 4\n",
    "\n",
    "# Call the decorator function using the @\n",
    "@cached(\n",
    "        # The func key or basename of file to save results to\n",
    "        'co_headwaters_reflectance_da_df')\n",
    "# Create new function to loop through each image, name da after the band, \n",
    "# apply the cloud mask, and store da by adding it as column in gdf, then concatenate\n",
    "def compute_reflectance_da(search_results, boundary_gdf):\n",
    "    \"\"\"\n",
    "    Connect to files over VSI, crop, cloud mask, and wrangle\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_df (search results) : pd.DataFrame\n",
    "        File connection and metadata (datetime, tile_id, band, and url)\n",
    "    boundary_gdf : gpd.GeoDataFrame\n",
    "        Boundary use to crop the data\n",
    "\n",
    "    Returns a single reflectance DataFrame with all bands as \n",
    "    columns and centroid coordinates and datetime as the index.\n",
    "    \"\"\"\n",
    "     # Create new function to crop raster\n",
    "    def open_dataarray(url, boundary_proj_gdf, masked=True):\n",
    "        \"\"\"\n",
    "        Open masked da, reproject CRS, and crop raster image\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        url: \n",
    "            the url to the raster image(s)\n",
    "        boundary_proj_gdf: gpd.GeoDataFrame\n",
    "            Boundary used to crop the data\n",
    "        scale=1: int\n",
    "            Scale factor to apply to the pixel values\n",
    "        masked=True: bool\n",
    "            Whether to mask the invalid or missing data\n",
    "        \n",
    "        Returns a cropped raster\n",
    "        \"\"\"\n",
    "        # Open masked DataArray\n",
    "        # Create variable for da, open raster file\n",
    "        da = rxr.open_rasterio(\n",
    "            # located at the url\n",
    "            url,\n",
    "            # ignore invlaid or missing data in calculations (masked=True)\n",
    "            mask_and_scale = masked\n",
    "            # Remove single dimensional axes from da to convert into scalar values\n",
    "            ).squeeze()\n",
    "        \n",
    "        # Reproject boundary if needed\n",
    "        # Check if boundary was already reprojected\n",
    "        if boundary_proj_gdf is None:\n",
    "            # If it hasn't been, reproject into CRS of raster data (da)\n",
    "            boundary_proj_gdf = boundary_gdf.to_crs(da.rio.crs)\n",
    "            \n",
    "        # Crop raster using the boundary\n",
    "        # Use coordinates from total bounds to clip the raster image(s)\n",
    "        # to the area defined by the boundary\n",
    "        cropped = da.rio.clip_box(\n",
    "            # Provide the bounding box of boundary's extent \n",
    "            # (mix, miny, maxx, maxy) via .total_bounds\n",
    "            *boundary_proj_gdf.total_bounds)\n",
    "\n",
    "        # Return cropped version of the raster\n",
    "        return cropped\n",
    "    \n",
    "    \n",
    "    # Create function to mask raster\n",
    "    def compute_quality_mask(da, mask_bits=[1, 2, 3]):\n",
    "        \"\"\"\n",
    "        Mask out low quality data by bit\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        da: xarray.DataArray\n",
    "            Raster data (to apply the mask to)\n",
    "        mask_bits[]: list of integers (optional)\n",
    "            List of bits positions that correspond to low-quality flags in the data \n",
    "            when these bits match these positions, the function will mask/ exclude \n",
    "            those values (clouds, cloud shadow, etc.) default is [1,2,3]\n",
    "\n",
    "        Returns masked raster\n",
    "        \"\"\"\n",
    "        # Unpack bits into a new axis\n",
    "        bits = (\n",
    "            # Takes 8 bit values and unpack into individual bits\n",
    "            np.unpackbits(\n",
    "                # Cast the input raster data (da) into 8 bit unassigned integers \n",
    "                # so that data is in a format that can be unpacked into individual bits\n",
    "                da.astype(np.uint8), \n",
    "                # Specify bit order, least significant bit at position 0\n",
    "                bitorder='little'\n",
    "            # Reshape resulting bit array to match the shape of the original \n",
    "            # raster (da) a 3D array format, and add new axis for unpacked bits\n",
    "            ).reshape(da.shape + (-1,))\n",
    "        )\n",
    "\n",
    "        # Select the required bits and check if any are flagged\n",
    "        mask = np.prod(\n",
    "            # Select specific bit positions, create boolean mask\n",
    "            bits[..., mask_bits]==0,\n",
    "            # Compute product of the elements along the chosen axis\n",
    "            # here chose the last axis of the array which is signaled \n",
    "            # using '-1'. \n",
    "            axis=-1)\n",
    "        # Return mask, which is binary where True/1 means the pixel \n",
    "        # is high quality versus False/0 means the pixel is low quality\n",
    "        return mask\n",
    "\n",
    "    # Create new variable for file df using get_earthaccess_links function\n",
    "    # to compile temporal and spatial information about each granule in gdfs\n",
    "    file_df = get_earthaccess_links(search_results)\n",
    "    clear_output()\n",
    "    # Creat empty list to save back to\n",
    "    granule_da_rows= []\n",
    "    # Create placeholder for variable name/ clear references to objects\n",
    "    boundary_proj_gdf = None\n",
    "\n",
    "    # Loop through each image\n",
    "    # Create new variable to - Group file_df by the columns 'datetime' \n",
    "    # and 'tile_id'. This will create a subset of data for each unique combo \n",
    "    # of datetime and tile_id\n",
    "    group_iter = file_df.groupby(['datetime', 'tile_id'])\n",
    "    # Loop through each group of datetime tile_id unique combos\n",
    "    # to provide a tuple containing the datetime and tile_id\n",
    "    # Use tqdm function for progress bar\n",
    "    for (datetime, tile_id), granule_df in tqdm(group_iter):\n",
    "        # Print message to show which granule is being processed \n",
    "        # based on tile_id and datetime\n",
    "        print(f'Processing granule {tile_id} {datetime}')\n",
    "              \n",
    "        # Open granule cloud cover\n",
    "        # Create new variable for the cloud mask url\n",
    "        cloud_mask_url = (\n",
    "            # Extract the cloud mask - band name is 'Fmask' \n",
    "            # from the granule_df \n",
    "            granule_df.loc[granule_df.band=='Fmask', 'url']\n",
    "            #\n",
    "            .values[0])\n",
    "        # Open cloud mask data,\n",
    "        cloud_mask_cropped_da = open_dataarray(\n",
    "            # Using boundary_proj_gdf as the spatial boundary cropping the data\n",
    "            cloud_mask_url, boundary_proj_gdf, \n",
    "            # Do not mask data initially\n",
    "            masked=False)\n",
    "\n",
    "        # Compute cloud mask\n",
    "        # Create new variable to apply the quality mask to \n",
    "        # identify which pixels are valid (not low quality)\n",
    "        cloud_mask = compute_quality_mask(cloud_mask_cropped_da)\n",
    "\n",
    "        # Loop through each spectral band\n",
    "        # Create empty list to store da's\n",
    "        da_list = []\n",
    "        # Create empty list to store df rows\n",
    "        df_list = []\n",
    "        # Loop through each row (spectral band) in granule_df\n",
    "        for i, row in granule_df.iterrows():\n",
    "            # If the spectral band name starts with 'B' ...\n",
    "            if row.band.startswith('B'):\n",
    "                # Open, crop, and mask the band\n",
    "                band_cropped = open_dataarray(\n",
    "                    # retrieve url for the band from the row\n",
    "                    row.url, \n",
    "                    # based pn the boundary_proj_gdf\n",
    "                    boundary_proj_gdf\n",
    "                    )\n",
    "                # Set band name\n",
    "                band_cropped.name = row.band\n",
    "                # Add the DataArray to the metadata DataFrame row\n",
    "                # and apply cloud mask using .where() - returns only \n",
    "                # non masked pixels\n",
    "                row['da'] = band_cropped.where(cloud_mask)\n",
    "                # Accumulate all processed rows for the granule\n",
    "                granule_da_rows.append(\n",
    "                    # Convert row to df\n",
    "                    row.to_frame().T)\n",
    "    \n",
    "    # Reassemble the metadata DataFrame\n",
    "    return pd.concat(granule_da_rows)\n",
    "\n",
    "# Apply the 'compute reflectance da' function\n",
    "\n",
    "# Create new data array df using the compute_reflectance_da \n",
    "# function to connect to files over VSI, crop, cloud mask, \n",
    "# and wrangle data\n",
    "co_reflectance_da_df = compute_reflectance_da(\n",
    "    # List of HLS files that are bounded by geographic \n",
    "    # bounds of the co_hls_gdf\n",
    "    orig_co_hls_results,\n",
    "    # CGY watershed boundary gdf\n",
    "    co_headwaters_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new variable for file df using get_earthaccess_links function\n",
    "# to compile temporal and spatial information about each granule in gdfs\n",
    "file_df = get_earthaccess_links(search_results)\n",
    "\n",
    "# Get the total number of tiles for overall progress bar\n",
    "total_tiles = len(file_df.groupby(['datetime', 'tile_id']))\n",
    "\n",
    "# Create placeholder for variable name/ clear references to objects\n",
    "boundary_proj_gdf = None\n",
    "\n",
    "# Create a single overall progress bar for the entire dataset\n",
    "with tqdm(total=total_tiles, desc=\"Processing Tiles\") as overall_progress:\n",
    "    # Loop through each image\n",
    "    # Create new variable to - Group file_df by the columns 'datetime' \n",
    "    # and 'tile_id'. This will create a subset of data for each unique combo \n",
    "    # of datetime and tile_id\n",
    "    group_iter = file_df.groupby(['datetime', 'tile_id'])\n",
    "\n",
    "    # Loop through each group of datetime tile_id unique combos\n",
    "    # to provide a tuple containing the datetime and tile_id\n",
    "    for (datetime, tile_id), granule_df in group_iter:\n",
    "        # Print message to show which granule is being processed \n",
    "        # based on tile_id and datetime\n",
    "        print(f'Processing granule {tile_id} {datetime}')\n",
    "              \n",
    "        # Open granule cloud cover\n",
    "        # Create new variable for the cloud mask url\n",
    "        cloud_mask_url = (\n",
    "            # Extract the cloud mask - band name is 'Fmask' \n",
    "            # from the granule_df \n",
    "            granule_df.loc[granule_df.band == 'Fmask', 'url']\n",
    "            .values[0])\n",
    "\n",
    "        # Open cloud mask data,\n",
    "        cloud_mask_cropped_da = open_dataarray(\n",
    "            # Using boundary_proj_gdf as the spatial boundary cropping the data\n",
    "            cloud_mask_url, boundary_proj_gdf, \n",
    "            # Do not mask data initially\n",
    "            masked=False)\n",
    "\n",
    "        # Compute cloud mask\n",
    "        # Create new variable to apply the quality mask to \n",
    "        # identify which pixels are valid (not low quality)\n",
    "        cloud_mask = compute_quality_mask(cloud_mask_cropped_da)\n",
    "\n",
    "        # Loop through each spectral band\n",
    "        # Create empty list to store da's\n",
    "        da_list = []\n",
    "        # Create empty list to store df rows\n",
    "        df_list = []\n",
    "        # Loop through each row (spectral band) in granule_df\n",
    "        for i, row in granule_df.iterrows():\n",
    "            # If the spectral band name starts with 'B' ...\n",
    "            if row.band.startswith('B'):\n",
    "                # Open, crop, and mask the band\n",
    "                band_cropped = open_dataarray(\n",
    "                    # retrieve url for the band from the row\n",
    "                    row.url, \n",
    "                    # based pn the boundary_proj_gdf\n",
    "                    boundary_proj_gdf, \n",
    "                    # Scale pixel values to proper unit (reflectance)\n",
    "                    scale=0.0001)\n",
    "                # Set band name\n",
    "                band_cropped.name = row.band\n",
    "                # Add the DataArray to the metadata DataFrame row\n",
    "                # and apply cloud mask using .where() - returns only \n",
    "                # non masked pixels\n",
    "                row['da'] = band_cropped.where(cloud_mask)\n",
    "                # Accumulate all processed rows for the granule\n",
    "                granule_da_rows.append(\n",
    "                    # Convert row to df\n",
    "                    row.to_frame().T)\n",
    "        \n",
    "        # Update the progress bar after each granule processing\n",
    "        overall_progress.update(1)\n",
    "\n",
    "# Reassemble the metadata DataFrame\n",
    "return pd.concat(granule_da_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle raster data Part 3 of 4\n",
    "\n",
    "# Call the decorator function using the @\n",
    "@cached(\n",
    "        # The func key or basename of file to save results to\n",
    "        'co_reflectance_da_df')\n",
    "# Create new function to loop through each image, name da after the band, \n",
    "# apply the cloud mask, and store da by adding it as column in gdf, then concatenate\n",
    "def compute_reflectance_da(search_results, boundary_gdf):\n",
    "    \"\"\"\n",
    "    Connect to files over VSI, crop, cloud mask, and wrangle\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_df (search results) : pd.DataFrame\n",
    "        File connection and metadata (datetime, tile_id, band, and url)\n",
    "    boundary_gdf : gpd.GeoDataFrame\n",
    "        Boundary use to crop the data\n",
    "\n",
    "    Returns a single reflectance DataFrame with all bands as \n",
    "    columns and centroid coordinates and datetime as the index.\n",
    "    \"\"\"\n",
    "     # Create new function to crop raster\n",
    "    def open_dataarray(url, boundary_proj_gdf, scale=1, masked=True):\n",
    "        \"\"\"\n",
    "        Open masked da, reproject CRS, and crop raster image\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        url: \n",
    "            the url to the raster image(s)\n",
    "        boundary_proj_gdf: gpd.GeoDataFrame\n",
    "            Boundary used to crop the data\n",
    "        scale=1: int\n",
    "            Scale factor to apply to the pixel values\n",
    "        masked=True: bool\n",
    "            Whether to mask the invalid or missing data\n",
    "        \n",
    "        Returns a cropped raster\n",
    "        \"\"\"\n",
    "        # Open masked DataArray\n",
    "        # Create variable for da, open raster file\n",
    "        da = rxr.open_rasterio(\n",
    "            # located at the url\n",
    "            url,\n",
    "            # ignore invlaid or missing data in calculations (masked=True)\n",
    "            masked=masked\n",
    "            # Remove single dimensional axes from da to convert into scalar values\n",
    "            ).squeeze(\n",
    "            # Multiply all pixel values in the da by a scaling factor   \n",
    "            ) * scale\n",
    "        \n",
    "        # Reproject boundary if needed\n",
    "        # Check if boundary was already reprojected\n",
    "        if boundary_proj_gdf is None:\n",
    "            # If it hasn't been, reproject into CRS of raster data (da)\n",
    "            boundary_proj_gdf = boundary_gdf.to_crs(da.rio.crs)\n",
    "            \n",
    "        # Crop raster using the boundary\n",
    "        # Use coordinates from total bounds to clip the raster image(s)\n",
    "        # to the area defined by the boundary\n",
    "        cropped = da.rio.clip_box(\n",
    "            # Provide the bounding box of boundary's extent \n",
    "            # (mix, miny, maxx, maxy) via .total_bounds\n",
    "            *boundary_proj_gdf.total_bounds)\n",
    "\n",
    "        # Return cropped version of the raster\n",
    "        return cropped\n",
    "    \n",
    "    \n",
    "    # Create function to mask raster\n",
    "    def compute_quality_mask(da, mask_bits=[1, 2, 3]):\n",
    "        \"\"\"\n",
    "        Mask out low quality data by bit\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        da: xarray.DataArray\n",
    "            Raster data (to apply the mask to)\n",
    "        mask_bits[]: list of integers (optional)\n",
    "            List of bits positions that correspond to low-quality flags in the data \n",
    "            when these bits match these positions, the function will mask/ exclude \n",
    "            those values (clouds, cloud shadow, etc.) default is [1,2,3]\n",
    "\n",
    "        Returns masked raster\n",
    "        \"\"\"\n",
    "        # Unpack bits into a new axis\n",
    "        bits = (\n",
    "            # Takes 8 bit values and unpack into individual bits\n",
    "            np.unpackbits(\n",
    "                # Cast the input raster data (da) into 8 bit unassigned integers \n",
    "                # so that data is in a format that can be unpacked into individual bits\n",
    "                da.astype(np.uint8), \n",
    "                # Specify bit order, least significant bit at position 0\n",
    "                bitorder='little'\n",
    "            # Reshape resulting bit array to match the shape of the original \n",
    "            # raster (da) a 3D array format, and add new axis for unpacked bits\n",
    "            ).reshape(da.shape + (-1,))\n",
    "        )\n",
    "\n",
    "        # Select the required bits and check if any are flagged\n",
    "        mask = np.prod(\n",
    "            # Select specific bit positions, create boolean mask\n",
    "            bits[..., mask_bits]==0,\n",
    "            # Compute product of the elements along the chosen axis\n",
    "            # here chose the last axis of the array which is signaled \n",
    "            # using '-1'. \n",
    "            axis=-1)\n",
    "        # Return mask, which is binary where True/1 means the pixel \n",
    "        # is high quality versus False/0 means the pixel is low quality\n",
    "        return mask\n",
    "\n",
    "    # Create new variable for file df using get_earthaccess_links function\n",
    "    # to compile temporal and spatial information about each granule in gdfs\n",
    "    file_df = get_earthaccess_links(search_results)\n",
    "    # Creat empty list to save back to\n",
    "    granule_da_rows= []\n",
    "    # Create placeholder for variable name/ clear references to objects\n",
    "    boundary_proj_gdf = None\n",
    "\n",
    "    # Loop through each image\n",
    "    # Create new variable to - Group file_df by the columns 'datetime' \n",
    "    # and 'tile_id'. This will create a subset of data for each unique combo \n",
    "    # of datetime and tile_id\n",
    "    group_iter = file_df.groupby(['datetime', 'tile_id'])\n",
    "    # Loop through each group of datetime tile_id unique combos\n",
    "    # to provide a tuple containing the datetime and tile_id\n",
    "    # Use tqdm function for progress bar\n",
    "    for (datetime, tile_id), granule_df in tqdm(group_iter):\n",
    "        # Print message to show which granule is being processed \n",
    "        # based on tile_id and datetime\n",
    "        print(f'Processing granule {tile_id} {datetime}')\n",
    "              \n",
    "        # Open granule cloud cover\n",
    "        # Create new variable for the cloud mask url\n",
    "        cloud_mask_url = (\n",
    "            # Extract the cloud mask - band name is 'Fmask' \n",
    "            # from the granule_df \n",
    "            granule_df.loc[granule_df.band=='Fmask', 'url']\n",
    "            #\n",
    "            .values[0])\n",
    "        # Open cloud mask data,\n",
    "        cloud_mask_cropped_da = open_dataarray(\n",
    "            # Using boundary_proj_gdf as the spatial boundary cropping the data\n",
    "            cloud_mask_url, boundary_proj_gdf, \n",
    "            # Do not mask data initially\n",
    "            masked=False)\n",
    "\n",
    "        # Compute cloud mask\n",
    "        # Create new variable to apply the quality mask to \n",
    "        # identify which pixels are valid (not low quality)\n",
    "        cloud_mask = compute_quality_mask(cloud_mask_cropped_da)\n",
    "\n",
    "        # Loop through each spectral band\n",
    "        # Create empty list to store da's\n",
    "        da_list = []\n",
    "        # Create empty list to store df rows\n",
    "        df_list = []\n",
    "        # Loop through each row (spectral band) in granule_df\n",
    "        for i, row in granule_df.iterrows():\n",
    "            # If the spectral band name starts with 'B' ...\n",
    "            if row.band.startswith('B'):\n",
    "                # Open, crop, and mask the band\n",
    "                band_cropped = open_dataarray(\n",
    "                    # retrieve url for the band from the row\n",
    "                    row.url, \n",
    "                    # based pn the boundary_proj_gdf\n",
    "                    boundary_proj_gdf, \n",
    "                    # Scale pixel values to proper unit (reflectance)\n",
    "                    scale=0.0001)\n",
    "                # Set band name\n",
    "                band_cropped.name = row.band\n",
    "                # Add the DataArray to the metadata DataFrame row\n",
    "                # and apply cloud mask using .where() - returns only \n",
    "                # non masked pixels\n",
    "                row['da'] = band_cropped.where(cloud_mask)\n",
    "                # Accumulate all processed rows for the granule\n",
    "                granule_da_rows.append(\n",
    "                    # Convert row to df\n",
    "                    row.to_frame().T)\n",
    "    \n",
    "    # Reassemble the metadata DataFrame\n",
    "    return pd.concat(granule_da_rows)\n",
    "\n",
    "# Apply the 'compute reflectance da' function\n",
    "\n",
    "# Create new data array df using the compute_reflectance_da \n",
    "# function to connect to files over VSI, crop, cloud mask, \n",
    "# and wrangle data\n",
    "co_reflectance_da_df = compute_reflectance_da(\n",
    "    # List of HLS files that are bounded by geographic \n",
    "    # bounds of the co_hls_gdf\n",
    "    co_hls_results,\n",
    "    # CGY watershed boundary gdf\n",
    "    cgy_headwaters_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call reflectance_da_df to see it\n",
    "co_reflectance_da_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrangle raster data Part 4 of 4\n",
    "# Merge and composite data\n",
    "\n",
    "# Call decorater function using @\n",
    "@cached(\n",
    "        # The func key or basename of file to save results to\n",
    "        'co_headwaters_reflectance_da')\n",
    "\n",
    "# Create new function to merge and compsite arrays\n",
    "def merge_and_composite_arrays(granule_da_df):\n",
    "    \"\"\"\n",
    "    Merge and composite and image for each band and composite \n",
    "    images across dates\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    granule_da_df : xarray.DataArray, pd.DataFrame\n",
    "       Multidimensional array /File connection and metadata\n",
    "\n",
    "     Returns merged and composite dataset into a single da\n",
    "    \"\"\"\n",
    "    # Merge and composite an image for each band\n",
    "    # Create empty list to store df's\n",
    "    df_list = []\n",
    "    # Create empty list to store composited da's\n",
    "    da_list = []\n",
    "    # Loop through each spectral band\n",
    "    # Use tqdm function to show progress bar\n",
    "    for band, band_df in tqdm(granule_da_df.groupby('band')):\n",
    "        # Create empty list to store merged images for all dates\n",
    "        merged_das = []\n",
    "        # Loop through each date group to merge granules for that date\n",
    "        # Use tqdm function to show progress bar\n",
    "        # Group band_df by datetime column\n",
    "        for datetime, date_df in tqdm(band_df.groupby('datetime')):\n",
    "            # Merge granules for each date into a single da representing the \n",
    "            #merged image for that date\n",
    "            merged_da = rxrmerge.merge_arrays(list(date_df.da))\n",
    "            # Mask negative values - removes neg values from the merged da\n",
    "            merged_da = merged_da.where(merged_da>0)\n",
    "            # Append this merged and cleaned da to to merged_das which \n",
    "            # holds the merged images for all dates\n",
    "            merged_das.append(merged_da)\n",
    "            \n",
    "        # Composite images across dates\n",
    "        composite_da = xr.concat(merged_das, dim='datetime').median('datetime')\n",
    "        # Extract band number from the band string and convert into an integer\n",
    "        composite_da['band'] = int(band[1:])\n",
    "        # Name the composite_da 'reflectance'\n",
    "        composite_da.name = 'reflectance'\n",
    "        # Append composited da to the da_list\n",
    "        da_list.append(composite_da)\n",
    "\n",
    "    # Return final merged and composited dataset\n",
    "    # Concatenate all composited da's along the band dimension using \n",
    "    # xr.concat, resulting in a single da\n",
    "    return xr.concat(da_list, dim='band')\n",
    "\n",
    "# Create new variable to use the merge_and_composite_arrays function\n",
    "co_reflectance_da = merge_and_composite_arrays(co_reflectance_da_df)\n",
    "# Call the variable to see it\n",
    "co_reflectance_da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert spectral DataArray to a tidy DataFrame\n",
    "cgy_model_df = co_reflectance_da.to_dataframe(\n",
    "    # Select the reflectance column from the df\n",
    "    # Reshape the df - create new column for each band and each \n",
    "    # row would correspond to a spatial location/pixel in the image\n",
    "    ).reflectance.unstack('band')\n",
    "# Overwrite model_df to drop\n",
    "cgy_model_df = cgy_model_df.drop(\n",
    "    # Columns 10 and 11 because they are not needed\n",
    "    columns=[10, 11]\n",
    "    #  and remove NaN values\n",
    "    ).dropna()\n",
    "\n",
    "# Running the fit and predict functions at the same time.\n",
    "# We can do this since we don't have target data.\n",
    "cgy_prediction = KMeans(\n",
    "    # use 6 clusters\n",
    "    n_clusters=6\n",
    "    # Fits model to data, then predicts the cluster for each row\n",
    "    ).fit_predict(\n",
    "        # Retrieve all values stored in the model_df\n",
    "        cgy_model_df.values)\n",
    "\n",
    "# Add the predicted values back to the model DataFrame\n",
    "cgy_model_df['clusters'] = cgy_prediction\n",
    "# Call the variable to see it\n",
    "cgy_model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try tuning the hyperparameter via silhoutette scores\n",
    "\n",
    "# Accumulate silhouette scores\n",
    "# Make an empty list to accumulate to\n",
    "cgy_silhouette = []\n",
    "# Make a list of K values to loop through, try 2 through 9\n",
    "cgy_k_list_values = list(range(2,9))\n",
    "# Loop through K values, use tqdm to wrap around loop to see progress bar\n",
    "for k in tqdm(cgy_k_list_values, desc=\"KMeans Progress\", ncols=100):\n",
    "    # Make model with K clusters\n",
    "    cgy_k_means = KMeans(n_clusters = k,\n",
    "    # Fits model to data\n",
    "    ).fit(\n",
    "        # Retrieve all values stored in the model_df\n",
    "        cgy_model_df.values)\n",
    "    \n",
    "    # Add cluster labels to df\n",
    "    cgy_model_df['k_means_labels'] = cgy_k_means.labels_\n",
    "    # Calculate silhouette score\n",
    "    cgy_score = silhouette_score(cgy_model_df.values, cgy_k_means.labels_)\n",
    "\n",
    "    # Calculate silhouette score and add it to list initialized \n",
    "    # with corresponding K values\n",
    "    cgy_silhouette.append(cgy_score)\n",
    "\n",
    "# Print silhouette scores for each k value\n",
    "#print(silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize silhouette scores as a scatterplot\n",
    "sns.scatterplot(\n",
    "    # Set x axis equal to the k_list_values\n",
    "    x = cgy_k_list_values,\n",
    "    # Set the y axis equal to the silhouette scores\n",
    "    y = cgy_silhouette\n",
    ")\n",
    "# Set the title using matplotlib\n",
    "plt.title('Colorado-Gunnison-Yampa Silhouette Scores vs. Number of Clusters')\n",
    "\n",
    "# Set axis labels\n",
    "# Label for the x-axis\n",
    "plt.xlabel('Number of Clusters (k)')  \n",
    "# Label for the y-axis\n",
    "plt.ylabel('Silhouette Score')        \n",
    "\n",
    "# Save as a jpg\n",
    "plt.savefig('cgy_silhouette_scores_vs_clusters.jpg') \n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will stick with 6 clusters because there is minimal difference in \n",
    "silhouette scores between 6 and 7. Additionally the more clusters \n",
    "the less clear the results will be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the k-means clusters\n",
    "\n",
    "# Create da with just rgb bands selected\n",
    "cgy_rgb = co_reflectance_da.sel(band=[4, 3, 2])\n",
    "# Change pixel values to 8 bit integers with range from 0-255\n",
    "# instead of 0-1 like the scale of the reflectance values\n",
    "cgy_rgb_uint8 = (cgy_rgb * 255\n",
    "    # Convert type\n",
    "    ).astype(np.uint8\n",
    "    # Take out NaN values\n",
    "    ).where(cgy_rgb!=np.nan)\n",
    "# Increase brightness of the image so it's not so dark\n",
    "cgy_rgb_bright = cgy_rgb_uint8 * 10\n",
    "# When rgb value is saterated, if value is 255 keep that value,\n",
    "# if value greater than 255, replace with 255\n",
    "cgy_rgb_sat = cgy_rgb_bright.where(cgy_rgb_bright < 255, 255)\n",
    "\n",
    "# Retrieve the CRS of the raster data (if it's set)\n",
    "# Assuming `rgb_sat` has the same CRS as `reflectance_da`\n",
    "# Assuming the CRS is stored in rasterio's CRS\n",
    "#raster_crs = reflectance_da.rio.crs \n",
    "\n",
    "# Reproject watershed boundary to match the raster CRS\n",
    "#ms_delta_gdf = ms_delta_gdf.to_crs(raster_crs)\n",
    "\n",
    "# Create variable name for plots in order to save later\n",
    "cgy_true_color_and_landcover_cluster_plots = (\n",
    "#true_color_image = (\n",
    "    # Plot using hvplot and .rgb method\n",
    "    cgy_rgb_sat.hvplot.rgb( \n",
    "        # Tell .rgb method to set the x axis = x, y axis = y,\n",
    "        # and bands = 'band'\n",
    "        x ='x', y ='y', bands = 'band',\n",
    "        # Avoid distortion\n",
    "        data_aspect=1,\n",
    "        # Drop the axes\n",
    "        xaxis = None, yaxis = None,\n",
    "        # Set width and height\n",
    "        width=600, height=600, \n",
    "        # Set title\n",
    "        title = 'Colorado, Gunnison, Yampa Headwater Areas - True Color Image')\n",
    "\n",
    "    # Add watershed boundary to the true color image\n",
    "    #+ ms_delta_gdf.hvplot(\n",
    "        # Set line color\n",
    "        #color = 'orange',\n",
    "        # Set line width\n",
    "        #line_width = 3,\n",
    "        #geo = raster_crs\n",
    "    #)\n",
    "#)\n",
    "+\n",
    "#landcover_clusters_plot = (\n",
    "    # Plot clusters, make into xarray\n",
    "    cgy_model_df.clusters.to_xarray(\n",
    "        # Tell how things are supposed to be realted to each other in the array\n",
    "        ).sortby(['x', 'y']).hvplot(\n",
    "        # Set color palette/scheme to colorblind\n",
    "        cmap = \"Colorblind\", \n",
    "        # Scale the x and y axis the same (helps prevent distortion)\n",
    "        aspect = 'equal',\n",
    "        # Drop the axes\n",
    "        xaxis = None, yaxis = None,\n",
    "        # Set width and height\n",
    "        width=600, height=600, \n",
    "        # Set title\n",
    "        title = 'Colorado, Gunnison, Yampa Headwater Areas - Land Cover Clusters',\n",
    "        # Set label for colorbar\n",
    "        clabel='Cluster',)\n",
    "\n",
    "    # Add watershed boundary to the cluster image    \n",
    "    #+ ms_delta_gdf.hvplot(\n",
    "        # Set line color\n",
    "        #color = 'orange',\n",
    "        # Set line width\n",
    "        #line_width = 3\n",
    "    #)\n",
    ")\n",
    "\n",
    "# Save the plot as html to be able to display online\n",
    "hv.save(cgy_true_color_and_landcover_cluster_plots, 'cgy_side_by_sides.html')  \n",
    "# Display the plot\n",
    "cgy_true_color_and_landcover_cluster_plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Many land cover types present - Clusters seem appropriate when comparing to True Color Image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earth-analytics-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
